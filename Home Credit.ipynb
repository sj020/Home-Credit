{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.317571Z",
     "iopub.status.busy": "2022-12-13T18:13:52.317114Z",
     "iopub.status.idle": "2022-12-13T18:13:52.349606Z",
     "shell.execute_reply": "2022-12-13T18:13:52.348465Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.317480Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function's\n",
    "    These are all the funtions used in the model preparation and model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.353289Z",
     "iopub.status.busy": "2022-12-13T18:13:52.352100Z",
     "iopub.status.idle": "2022-12-13T18:13:52.383644Z",
     "shell.execute_reply": "2022-12-13T18:13:52.382201Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.353241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper Methods\n",
    "def checking_missing_values(data):\n",
    "    count = data.isnull().sum().sort_values(ascending=False)\n",
    "    percentage = ((data.isnull().sum()/len(data)*100)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([count, percentage], axis=1, keys=['Count','Percentage'])\n",
    "    return missing_data\n",
    "\n",
    "# Dropping the Missing values if greater than 40%\n",
    "def dropping_missing_values(data):\n",
    "    percentage = 40.0\n",
    "    min_count = int(((100-percentage)/100)*data.shape[0] + 1)\n",
    "    dropped_data = data.dropna(axis = 1, thresh = min_count)\n",
    "    return dropped_data\n",
    "\n",
    "# Checking for Duplicate data\n",
    "def checking_duplicate_data(data):\n",
    "    columns = []\n",
    "    for col in data.columns:\n",
    "        if col!='SK_ID_CURR':\n",
    "            columns.append(col)\n",
    "    flag = data[data.duplicated(subset = columns, keep=False)]\n",
    "    return flag\n",
    "\n",
    "# Imputing categorical missing values with mode\n",
    "def imputing_categorical_missing_values(data):\n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        if col_type == object:\n",
    "            data[col] = data[col].fillna(data[col].mode().iloc[0])\n",
    "    return data\n",
    "\n",
    "# One Hot Encoding the categorical variables\n",
    "def encoding_categorical(data):\n",
    "    # Dropping the first column to not get into dummy variable trap\n",
    "    data = pd.get_dummies(data, drop_first = True)\n",
    "    return data\n",
    "\n",
    "# Mean Imputation To impute the missing values in numerical columns\n",
    "def mean_imputation(data):\n",
    "    mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    data = pd.DataFrame(mean_imputer.fit_transform(data), columns=data.columns)\n",
    "    return data\n",
    "\n",
    "# MICE Imputation :-> To impute the missing values in numerical columns\n",
    "def mice_imputation(data):\n",
    "    mice_imputer = IterativeImputer(estimator=linear_model.BayesianRidge())\n",
    "    df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(data), columns=data.columns)\n",
    "    return df_mice_imputed\n",
    "\n",
    "def IV_calculation(X, y, head = 100):\n",
    "    clf = WOE()\n",
    "    clf.fit(X, y)\n",
    "    iv_values = clf.iv_df.head(head)\n",
    "    return iv_values\n",
    "\n",
    "def tuning_splits(X, y):\n",
    "    n_splits = [5, 6, 7, 8, 9,10]\n",
    "    scores = []\n",
    "    for s in n_splits:\n",
    "        kf = StratifiedKFold(n_splits=s, shuffle=True, random_state=42)\n",
    "        score = cross_val_score(lm.LogisticRegression(), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "\n",
    "def tuning_C(X, y):\n",
    "    C = [0.001, 0.01, 0.1, 1, 10]\n",
    "    scores = []\n",
    "    kf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    for c in C:\n",
    "        score = cross_val_score(lm.LogisticRegression(C = c, random_state = 42), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "\n",
    "def tuning_solver(X, y):\n",
    "    algo = ['lbfgs', 'liblinear']\n",
    "    scores = []\n",
    "    kf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
    "    for alg in algo:\n",
    "        score = cross_val_score(lm.LogisticRegression(max_iter = 3000, solver = alg, random_state = 42), X, y, cv= kf, scoring=\"roc_auc\")\n",
    "        scores.append(np.mean(score))\n",
    "    return scores\n",
    "    \n",
    "\n",
    "def model(X, y, tuning = False):\n",
    "    if tuning == False:\n",
    "        logreg = LogisticRegression()\n",
    "    else:\n",
    "        logreg = LogisticRegression(C=0.01, class_weight = 'balanced', max_iter=200, solver = '')\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "    cv_scores = cross_validate(logreg, X, Y, n_jobs=-1, cv=kf, scoring ='roc_auc')\n",
    "    \n",
    "    return cv_scores, logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the required Libraries\n",
    "    Installing the libraries for calculating Information Value and Cramer's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:13:52.385896Z",
     "iopub.status.busy": "2022-12-13T18:13:52.385136Z",
     "iopub.status.idle": "2022-12-13T18:14:18.036896Z",
     "shell.execute_reply": "2022-12-13T18:14:18.035412Z",
     "shell.execute_reply.started": "2022-12-13T18:13:52.385858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xverse\n",
      "  Downloading xverse-1.0.5-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (1.21.5)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/anaconda3/lib/python3.9/site-packages (from xverse) (0.13.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=3.0.3->xverse) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21.1->xverse) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.0->xverse) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.0->xverse) (1.1.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/anaconda3/lib/python3.9/site-packages (from statsmodels>=0.6.1->xverse) (0.5.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels>=0.6.1->xverse) (1.16.0)\n",
      "Installing collected packages: xverse\n",
      "Successfully installed xverse-1.0.5\n",
      "Collecting association-metrics\n",
      "  Downloading association-metrics-0.0.1.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: association-metrics\n",
      "  Building wheel for association-metrics (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for association-metrics: filename=association_metrics-0.0.1-py3-none-any.whl size=3928 sha256=75ade96569ff1af6ceb436bf4c56178f7b691955a9d067ba16d4f0724f4c89f0\n",
      "  Stored in directory: /Users/sidhant/Library/Caches/pip/wheels/49/ef/10/d93c981055c4fc0401028bc9dfda085ddd5ca204bcda0f2110\n",
      "Successfully built association-metrics\n",
      "Installing collected packages: association-metrics\n",
      "Successfully installed association-metrics-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xverse\n",
    "!pip install association-metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:18.041600Z",
     "iopub.status.busy": "2022-12-13T18:14:18.041026Z",
     "iopub.status.idle": "2022-12-13T18:14:18.914163Z",
     "shell.execute_reply": "2022-12-13T18:14:18.912991Z",
     "shell.execute_reply.started": "2022-12-13T18:14:18.041542Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VSETIN~1\\AppData\\Local\\Temp/ipykernel_7232/640471794.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Model Selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scorer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFitFailedWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madjusted_mutual_info_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madjusted_rand_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_supervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfowlkes_mallows_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_supervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalinski_harabasz_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances_chunked\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msp_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pairwise_distances_reduction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPairwiseDistancesArgKmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pairwise_fast\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_chi2_kernel_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sparse_manhattan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m from ._dispatcher import (\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0mBaseDistancesReductionDispatcher\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mArgKmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBOOL_METRICS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMETRIC_MAPPING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_sqeuclidean_row_norms32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sqeuclidean_row_norms64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m from ._argkmin import (\n\u001b[0;32m     13\u001b[0m     \u001b[0mArgKmin64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_base.pyx\u001b[0m in \u001b[0;36minit sklearn.metrics._pairwise_distances_reduction._base\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.metrics._dist_metrics' has no attribute 'DistanceMetric32'"
     ]
    }
   ],
   "source": [
    "# Importing the Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Removing the Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Importing the Logestic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# For calculation of Weight of Evidence and Information Value\n",
    "from xverse.transformer import WOE\n",
    "\n",
    "# Cramers V\n",
    "import association_metrics as am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sklearn.metrics._dist_metrics (from versions: none)\n",
      "ERROR: No matching distribution found for sklearn.metrics._dist_metrics\n"
     ]
    }
   ],
   "source": [
    "sklearn.metrics._dist_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:18.916350Z",
     "iopub.status.busy": "2022-12-13T18:14:18.915922Z",
     "iopub.status.idle": "2022-12-13T18:14:25.609191Z",
     "shell.execute_reply": "2022-12-13T18:14:25.608061Z",
     "shell.execute_reply.started": "2022-12-13T18:14:18.916305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Dataset\n",
    "df = pd.read_csv(\"/kaggle/input/home-credit-default-risk/application_train.csv\")\n",
    "print('The shape of data:',df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Preprocessing\n",
    "    In this we gonna check for missing value's, dropping the missing value column's, checking for duplicate value's and treating the anomalies.\n",
    "    Also, we will impute the missing value's using Mean as well as MICE Imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in e:\\anaconda\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in e:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in e:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in e:\\anaconda\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in e:\\anaconda\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:25.612153Z",
     "iopub.status.busy": "2022-12-13T18:14:25.611720Z",
     "iopub.status.idle": "2022-12-13T18:14:26.161368Z",
     "shell.execute_reply": "2022-12-13T18:14:26.160587Z",
     "shell.execute_reply.started": "2022-12-13T18:14:25.612111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for Missing Values\n",
    "checking_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:26.163228Z",
     "iopub.status.busy": "2022-12-13T18:14:26.162886Z",
     "iopub.status.idle": "2022-12-13T18:14:26.542913Z",
     "shell.execute_reply": "2022-12-13T18:14:26.541685Z",
     "shell.execute_reply.started": "2022-12-13T18:14:26.163198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing those columns which are having more than 40% of missing values\n",
    "df = dropping_missing_values(df)\n",
    "\n",
    "# Shape of application after dropping the columns having more than 40% missing values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:26.544659Z",
     "iopub.status.busy": "2022-12-13T18:14:26.544305Z",
     "iopub.status.idle": "2022-12-13T18:14:28.837427Z",
     "shell.execute_reply": "2022-12-13T18:14:28.835660Z",
     "shell.execute_reply.started": "2022-12-13T18:14:26.544627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for duplicate data\n",
    "temp = checking_duplicate_data(df)\n",
    "print(f\"The no. of duplicates in the data: {temp.shape[0]}\\n\")\n",
    "\n",
    "print(f\"Cleaning Started....\\n\")\n",
    "# Cleaning columns as some columns contain XNA instead of NA and some are having typing mistake\n",
    "## Column -> CODE_GENDER\n",
    "df['CODE_GENDER'] = df['CODE_GENDER'].replace(\"XNA\", np.nan)\n",
    "## Column -> ORGANIZATION_TYPE\n",
    "df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].replace(\"XNA\", \"Other\")\n",
    "## Column -> DAYS_EMPLOYED\n",
    "df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].replace(365243, np.nan)\n",
    "print(f\"Cleaning done!!!\")\n",
    "\n",
    "# Getting application without ID and Target\n",
    "application_without_ID_Target = df.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "\n",
    "print(\"Imputing and Encoding Categorical Variables with Mode....\\n\")\n",
    "\n",
    "# Imputing the categorical Values with Mode\n",
    "application_without_ID_Target = imputing_categorical_missing_values(application_without_ID_Target)\n",
    "\n",
    "# Encoding the Categorical Columns\n",
    "application_without_ID_Target = encoding_categorical(application_without_ID_Target)\n",
    "\n",
    "# Imputing with Mode as it is categorical\n",
    "application_without_ID_Target['CNT_FAM_MEMBERS'] = application_without_ID_Target['CNT_FAM_MEMBERS'].fillna(application_without_ID_Target['CNT_FAM_MEMBERS'].mode().iloc[0])\n",
    "\n",
    "print(\"Imputing and Encoding Done!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:14:28.839704Z",
     "iopub.status.busy": "2022-12-13T18:14:28.839333Z",
     "iopub.status.idle": "2022-12-13T18:51:18.770680Z",
     "shell.execute_reply": "2022-12-13T18:51:18.769599Z",
     "shell.execute_reply.started": "2022-12-13T18:14:28.839660Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Imputing Numeriacal Varaible Started.....\\n\")\n",
    "# print(\"Mean Imputation Started....\\n\")\n",
    "\n",
    "# # Imputing using Mean Strategy on numerical Columns\n",
    "# application_mean = mean_imputation(application_without_ID_Target)\n",
    "\n",
    "# print(\"Saving the Mean Imputed File.....\")\n",
    "# # Saving the final application after imputation into a csv\n",
    "# application_mean['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "# application_mean['TARGET'] = df['TARGET']\n",
    "# application_mean.to_csv('/kaggle/working/application_mean.csv',index=False)\n",
    "\n",
    "application_mean = pd.read_csv(\"application_mean.csv\") # Here read the file\n",
    "X_mean = application_mean.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_mean = application_mean['TARGET']\n",
    "# print(\"Saved!!!\")\n",
    "\n",
    "# print(f\"Mean Imputation Done!!!\\n\")\n",
    "\n",
    "# print(f\"MICE Imputation Started....\\n\")\n",
    "# application_mice = mice_imputation(application_without_ID_Target)\n",
    "\n",
    "# print(\"Saving the MICE imputed file....\")\n",
    "# # Saving the final application after imputation into a csv\n",
    "# application_mice['SK_ID_CURR'] = df['SK_ID_CURR']\n",
    "# application_mice['TARGET'] = df['TARGET']\n",
    "# application_mice.to_csv('/kaggle/working/application_mice.csv',index=False)\n",
    "\n",
    "application_mice = pd.read_csv(\"application_mice.csv\") # Here read the file\n",
    "X_mice = application_mice.drop(['SK_ID_CURR', 'TARGET'], axis = 1)\n",
    "y_mice = application_mice['TARGET']\n",
    "# print(\"Saved!!!\")\n",
    "\n",
    "# print(f\"MICE Imputation Done!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Checking if imputation didnt change the distribution of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T18:51:18.774126Z",
     "iopub.status.busy": "2022-12-13T18:51:18.773785Z",
     "iopub.status.idle": "2022-12-13T18:51:18.778978Z",
     "shell.execute_reply": "2022-12-13T18:51:18.777659Z",
     "shell.execute_reply.started": "2022-12-13T18:51:18.774095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Imputation result\n",
    "\n",
    "# MICE Imputation result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "    In this we would fir two model's for each category one would be the basic and another would be the tuned one. We will use Stratified KFold Cross Validation for that and as a performance metric we will use AUC(Area under Curve) ROC(Reciever Operating Characterstics) as it is highly imbalanced dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:05:36.086071Z",
     "iopub.status.busy": "2022-12-13T19:05:36.085663Z",
     "iopub.status.idle": "2022-12-13T19:05:36.094194Z",
     "shell.execute_reply": "2022-12-13T19:05:36.092995Z",
     "shell.execute_reply.started": "2022-12-13T19:05:36.086037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe to keep the track of all the results \n",
    "df_models = pd.DataFrame(columns = ['Model Name', 'Algorithm', 'Features', 'Target', 'Train AUC', 'Valid AUC', 'Test AUC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputed with no Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:05:38.788733Z",
     "iopub.status.busy": "2022-12-13T19:05:38.788303Z",
     "iopub.status.idle": "2022-12-13T19:06:26.130160Z",
     "shell.execute_reply": "2022-12-13T19:06:26.128784Z",
     "shell.execute_reply.started": "2022-12-13T19:05:38.788695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Imputed Model with no tuning\n",
    "print(f\"STARTING MEAN IMPUTATION NO TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mean, y_mean, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression()\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'Mean Imputed No Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mean.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MEAN IMPUTATION NO TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Imputed with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:06:26.133556Z",
     "iopub.status.busy": "2022-12-13T19:06:26.132761Z",
     "iopub.status.idle": "2022-12-13T19:06:26.140841Z",
     "shell.execute_reply": "2022-12-13T19:06:26.139428Z",
     "shell.execute_reply.started": "2022-12-13T19:06:26.133503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding the tuned parameters\n",
    "# print(\"Finding the regularization parameter.....\")\n",
    "# c_score = tuning_C(X, y)\n",
    "# print(\"Finding the Solver.....\")\n",
    "# solver = tuning_solver(X_mice, y_mice)\n",
    "# print(\"Solver found!!!!\")\n",
    "# print(\"Finding the correct k-folds.....\")\n",
    "# n_splits = tuning_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T19:06:26.143209Z",
     "iopub.status.busy": "2022-12-13T19:06:26.142650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean Imputed Model with tuning\n",
    "print(f\"STARTING MEAN IMPUTATION TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mean, y_mean, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression(C = 0.1, penalty = 'l1',random_state = 42, solver ='liblinear', max_iter= 3000, class_weight='balanced')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'Mean Imputed With Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mean.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MEAN IMPUTATION WITH TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE Imputed with no tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mice Imputed Model with no tuning\n",
    "print(f\"STARTING MICE IMPUTATION WITH NO TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mice, y_mice, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression()\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'MICE Imputed No Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mice.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MICE IMPUTATION WITH NO TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MICE Imputed with Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the tuned parameters\n",
    "# print(\"Finding the regularization parameter.....\")\n",
    "# c_score = tuning_C(X, y)\n",
    "# print(\"Finding the Solver.....\")\n",
    "# solver = tuning_solver(X_mice, y_mice)\n",
    "# print(\"Solver found!!!!\")\n",
    "# print(\"Finding the correct k-folds.....\")\n",
    "# n_splits = tuning_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mice Imputed Model with tuning\n",
    "print(f\"STARTING MICE IMPUTATION WITH TUNING MODEL FITTING>>>\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mice, y_mice, random_state = 42, test_size = 0.2)\n",
    "\n",
    "print(\"Started KFold Cross Validation.....\")\n",
    "logreg = LogisticRegression(C = 0.1, penalty = 'l1',random_state = 42, solver ='liblinear', max_iter= 3000, class_weight='balanced')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc',return_train_score = True)\n",
    "print(f\"KFold Cross Validation Ended!!!!\\n\")\n",
    "print(f\"Total time taken to fit the model is {np.sum(cv_scores['fit_time'])/60} min\\n\")\n",
    "\n",
    "print(\"Starting predicting on the Test data....\")\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "test_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"Prediction done on test data!!!!\\n\")\n",
    "\n",
    "print(f\"Appending the resulted Model in the Models Dataframe.....\")\n",
    "df_models = df_models.append({'Model Name':'MICE Imputed with Tuning',\n",
    "                             'Algorithm':'Logistic Regression',\n",
    "                             'Features':list(X_mice.columns.values),\n",
    "                             'Target':'0/1',\n",
    "                             'Train AUC':np.mean(cv_scores['train_score']),\n",
    "                             'Valid AUC':np.mean(cv_scores['test_score']),\n",
    "                             'Test AUC':test_auc}, ignore_index=True)\n",
    "print(f\"Model is added in the Models Dataframe!!!\\n\")\n",
    "\n",
    "print(f\"ENDED THE MICE IMPUTATION WITH TUNING MODEL FITTING!!!!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection in Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using IV\n",
    "print(\"IV Calculation begin.....\")\n",
    "iv_df = IV_calculation(X_mice, y_mice, 20)\n",
    "\n",
    "print(\"IV Calculation Done!!!!\")\n",
    "## Feature selected from IV are those between 0.03 to 0.1 are evaluated \n",
    "## and greater than 0.1 is Retained\n",
    "iv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all other columns\n",
    "application = application_mice[['EXT_SOURCE_3','EXT_SOURCE_2','DAYS_EMPLOYED','DAYS_BIRTH','AMT_GOODS_PRICE','AMT_CREDIT', \n",
    "                                'DAYS_ID_PUBLISH', 'DAYS_LAST_PHONE_CHANGE','DAYS_REGISTRATION', 'REGION_RATING_CLIENT', \n",
    "                                'REGION_RATING_CLIENT_W_CITY', 'TARGET','SK_ID_CURR',\n",
    "                                'NAME_EDUCATION_TYPE_Higher education']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the other datasets and Merging\n",
    "    Importing the other files and merging with the reduced application as we will try to increase the Models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Shape\n",
    "print(f'Initial Train Shape:{application.shape}')\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Combining Dataframes to Train\n",
    "def merge_train(df):\n",
    "    train_final = pd.merge(application, df, how='left', on = ['SK_ID_CURR'])\n",
    "    return train_final\n",
    "\n",
    "train = merge_train(bureau)\n",
    "print(f'New shape after bureau/bureau balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(POS_CASH_balance)\n",
    "print(f'New shape after POS_CASH_balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(installments_payments)\n",
    "print(f'New shape after installments_payments: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "train = merge_train(credit_card_balance)\n",
    "print(f'New shape after credit_card_balance: {train.shape}')\n",
    "print(\"\")\n",
    "\n",
    "final_df = merge_train(previous_application)\n",
    "print(f'New shape after previous_application: {final_df.shape}')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = dropping_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.drop(['SK_ID_CURR', 'TARGET', 'SK_ID_PREV'], axis = 1)\n",
    "y = final_df['TARGET']\n",
    "\n",
    "\n",
    "iv_df = IV_calculation(X, y, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['EXT_SOURCE_3', 'EXT_SOURCE_2', 'DAYS_EMPLOYED', 'DAYS_BIRTH','REGION_RATING_CLIENT_W_CITY', \n",
    "                    'PRODUCT_COMBINATION', 'REGION_RATING_CLIENT', 'CODE_REJECT_REASON', 'DAYS_LAST_PHONE_CHANGE','NAME_CONTRACT_STATUS',\n",
    "                    'NAME_EDUCATION_TYPE_Higher education', 'DAYS_ID_PUBLISH', 'AMT_GOODS_PRICE_x', 'AMT_CREDIT_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = imputing_categorical_missing_values(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encoding_categorical(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df\n",
    "y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=0.1, penalty='l1',class_weight = 'balanced', random_state=42, solver = 'liblinear')\n",
    "kf = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n",
    "cv_scores = cross_validate(logreg, X_train, y_train, n_jobs=-1, cv=kf, scoring ='roc_auc', return_train_score = True)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training ROC\": np.mean(cv_scores['train_score']))\n",
    "print(f\"Validating ROC: {np.mean(cv_scores['test_score'])}\")\n",
    "print(f\"Test ROC: {roc_auc_score(y_test, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
